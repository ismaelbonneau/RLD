{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 3: Q-learning\n",
    "\n",
    "_Ismaël Bonneau_\n",
    "\n",
    "Ce TP sert de compte-rendu pour le TP 3 et d'explication à la fois.\n",
    "\n",
    "Algorithmes implémentés:\n",
    "\n",
    "- Q-learning en version basique\n",
    "- Q-learning en version SARSA\n",
    "\n",
    "## Le Q-learning: associer une valeur à chaque couple (état, action)\n",
    "\n",
    "On définit une fonction ${Q}$ qui prend une valeur d'état ${s_t}$ et une valeur d'action ${a_t}$ (${s_t}$ signifie: un certain état ${s}$ au temps ${t}$). Cette fonction ${Q(s_t, a_t)}$ renvoie une valeur indiquant l'estimation que l'on fait de la récompense que l'on attend si, en partant de ${s}$ à l'instant ${t}$, on effectue l'action ${a}$.\n",
    "\n",
    "Cela peut permettre de dériver une politique greddy par exemple: Une fois la fonction ${Q}$ connue, on pourrait choisir, à chaque étape ${t}$ pour chaque état ${s_t}$ , d'effectuer l'action ${a}$ ayant la plus grande valeur ${Q(s_t, a)}$.  \n",
    "\n",
    "<img src=\"images/qlearning_principle.png\">\n",
    "\n",
    "Il faut bien entendu pour cela estimer la fonction ${Q}$. On va pour ça initialiser une fonction ${Q}$ (généralement 0 partout au début) et l'estimer en jouant. A chaque étape ${t}$, notre agent agit selon sa fonction ${Q}$ actuelle, en considérant l'état ${s_t}$ dans lequel il se trouve, puis se retrouve dans un nouvel état ${s_{t+1}}$ en ayant reçu une récompense ${r}$. Il va ensuite mettre à jour sa fonction ${Q}$ en utilisant cette règle:\n",
    "\n",
    "<img src=\"images/qlearning_updaterule.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gridworld\n",
    "from gym import wrappers, logger\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nous allons l'appliquer sur un jeu simple, GridWorld\n",
    "\n",
    "Il s'agit d'un petit jeu en labyrinthe simple dans lequel un personnage (carré) doit rejoindre un endroit d'arrivée et évitant les flammes. Un reward positif est donné à l'arrivée, un négatif au passage dans les flammes et un reward négatif très petit (${-0.01}$ par exemple est donné aux autres cases, cela pour limiter le temps passé à cheminer à traver le labyrinthe.\n",
    "\n",
    "Configurons l'environnement Gym pour gridworld-v0. Il a besoin de 2 choses:\n",
    "\n",
    "- Un plan (le numéro 4 par exemple, qui est déjà assez complexe)\n",
    "- Des valeurs associées à chaque type de case. 0 correspond à une case vide, 1 correspond à\n",
    "un mur (pas de reward associé car impossible de s'y déplacer), 2 correspond au joueur,\n",
    "3 correspond à une case verte, 4 une case jaune, 5 une case rouge et 6 une case rose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"gridworld-v0\")\n",
    "outdir = 'TP3/qlearning-agent-results'\n",
    "envm = wrappers.Monitor(env, directory=outdir, force=True, video_callable=False)\n",
    "env.setPlan(\"gridworldPlans/plan2.txt\", {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1}) # initialiser le plan\n",
    "env.seed(0)\n",
    "lol = env.render() #visualiser le plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nous allons définir deux classes d'agent\n",
    "\n",
    "Un agent prenant des décisions complètement aléatoire, et un agent faisant du Q-learning. Cela servira comme point de comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import operator\n",
    "\n",
    "class Qagent(object):\n",
    "    \"\"\" Q-learning agent \"\"\"\n",
    "    def __init__(self, env, gamma=0.99, alpha=0.8, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.Q = {} # initialement vide: on ne connait rien\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.last_action = None\n",
    "\n",
    "    def register_state(self, state):\n",
    "        if state not in self.Q:\n",
    "            self.Q[state] = {a: 0 for a in range(self.env.action_space.n)}\n",
    "                \n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\" On parle pas chinois ici \"\"\"\n",
    "        state = self.env.state2str(observation)\n",
    "        self.register_state(state)\n",
    "        # epsilon-greedy:\n",
    "        if random.random() < self.epsilon:\n",
    "            # sampler une action au hasard\n",
    "            chosen_action = self.env.action_space.sample()\n",
    "        else:\n",
    "            # sinon, effectuer l'action conforme à la politique\n",
    "            chosen_action = np.argmax(self.Q[state])\n",
    "\n",
    "        self.last_action = chosen_action\n",
    "        self.last_state = state\n",
    "        return chosen_action\n",
    "\n",
    "    def improve(self, observation, reward, done):\n",
    "        \"\"\" Hassoul y'a qu'le charbon qui paie \"\"\"\n",
    "        new_state = self.env.state2str(observation)\n",
    "        self.register_state(new_state)\n",
    "        LPBDTLR = self.gamma * max(self.Q[new_state].items(), key=operator.itemgetter(1))[1]\n",
    "        self.Q[self.last_state][self.last_action] = ((1 - self.alpha) * self.Q[self.last_state][self.last_action]) + self.alpha * (reward + LPBDTLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "EPSILON = 0.1\n",
    "\n",
    "agent = Qagent(env, gamma=GAMMA, alpha=LEARNING_RATE, epsilon=EPSILON)\n",
    "\n",
    "nbruns = 300\n",
    "\n",
    "for i in range(nbruns):\n",
    "    obs = envm.reset()\n",
    "    rsum = 0\n",
    "    reward = 0\n",
    "    done = False\n",
    "    j = 0\n",
    "    while True:\n",
    "        action = agent.act(obs, reward, done)\n",
    "        obs, reward, done, _ = envm.step(action)\n",
    "        agent.improve(obs, reward, done)\n",
    "        rsum = (rsum * GAMMA) + reward\n",
    "        j += 1\n",
    "        if done:\n",
    "            if i % 10 == 0:\n",
    "                print(\"episode\", i,\" rsum=\" + str(rsum) + \", au bout de \" + str(j) + \" actions\")\n",
    "            break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
