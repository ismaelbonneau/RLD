{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 3: Q-learning\n",
    "\n",
    "_Ismaël Bonneau_\n",
    "\n",
    "Ce TP sert de compte-rendu pour le TP 3 et d'explication à la fois.\n",
    "\n",
    "Algorithmes implémentés:\n",
    "\n",
    "- Q-learning en version basique\n",
    "- SARSA (State-Action-Reward-State-Action)\n",
    "\n",
    "Q-learning et SARSA diffèrent légèrement en pratique dans leur principe et leur implémentation. Ce sont tous les deux des méthodes model-free (pas besoin de connaitre les règles de l'environnement, AKA le MDP) et se basent sur une estimation de la fonction ${Q}$\n",
    "\n",
    "## Le Q-learning: associer une valeur à chaque couple (état, action)\n",
    "\n",
    "On définit une fonction ${Q}$ qui prend une valeur d'état ${s_t}$ et une valeur d'action ${a_t}$ (${s_t}$ signifie: un certain état ${s}$ au temps ${t}$). Cette fonction ${Q(s_t, a_t)}$ renvoie une valeur indiquant l'estimation que l'on fait de la récompense que l'on attend si, en partant de ${s}$ à l'instant ${t}$, on effectue l'action ${a}$.\n",
    "\n",
    "Cela peut permettre de dériver une politique greddy par exemple: Une fois la fonction ${Q}$ connue, on pourrait choisir, à chaque étape ${t}$ pour chaque état ${s_t}$ , d'effectuer l'action ${a}$ ayant la plus grande valeur ${Q(s_t, a)}$.  \n",
    "\n",
    "<img src=\"images/qlearning_principle.png\">\n",
    "\n",
    "Il faut bien entendu pour cela estimer la fonction ${Q}$. On va pour ça initialiser une fonction ${Q}$ (généralement 0 partout au début) et l'estimer en jouant. A chaque étape ${t}$, notre agent agit selon sa fonction ${Q}$ actuelle, en considérant l'état ${s_t}$ dans lequel il se trouve, puis se retrouve dans un nouvel état ${s_{t+1}}$ en ayant reçu une récompense ${r}$. Il va ensuite mettre à jour sa fonction ${Q}$ en utilisant cette règle:\n",
    "\n",
    "<img src=\"images/qlearning_updaterule.png\">\n",
    "\n",
    "\n",
    "## SARSA: une règle de mise à jour un peu différente\n",
    "\n",
    "Pour **SARSA**, le mode de mise à jour de ${Q}$ est légèrement différent. Pendant qu'avec le Q-learning, à chaque fois qu'on faisait une action et obtenait une récompense, la fonction ${Q}$ était remis à jour, en utilisant l'opérateur ${max}$ sur ce qu'on pourrait obtenir au coup suivant, SARSA n'utilise pas le ${max}$ mais plutôt une estimation de la politique en se basant sur ${Q}$.\n",
    "\n",
    "Pour bien comprendre:\n",
    "\n",
    " - Dans la version Q-learning, on part d'un état ${s_t}$, on choisit l'action ${a}$ à faire selon la politique dérivée de ${Q}$ (epsilon-greedy par ex) et on se retrouve dans un état ${s_{t+1}}$. On regarde ensuite le ${max_a Q(s_{t+1}, a)}$: cela nous donne une estimation de la meilleure récompense que l'on peut obtenir à partir de ${s_{t+1}}$.\n",
    " \n",
    " - Dans la version SARSA, on ne regarde pas ${max_a Q(s_{t+1}, a)}$, mais une fois dans l'état ${s_{t+1}}$, on refait un choix d'action selon la politique dérivée de ${Q}$. C'est cette valeur d'action ${a_{t+1}}$ que l'on va utiliser pour mettre à jour notre ${Q}$.\n",
    " \n",
    "D'où le nom **SARSA**: State-Action-Reward-State-Action\n",
    "\n",
    "<img src=\"images/sarsa_principle.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gridworld\n",
    "from gym import wrappers, logger\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nous allons l'appliquer sur un jeu simple, GridWorld\n",
    "\n",
    "Il s'agit d'un petit jeu en labyrinthe simple dans lequel un personnage (carré) doit rejoindre un endroit d'arrivée et évitant les flammes. Un reward positif est donné à l'arrivée, un négatif au passage dans les flammes et un reward négatif très petit (${-0.01}$ par exemple est donné aux autres cases, cela pour limiter le temps passé à cheminer à traver le labyrinthe.\n",
    "\n",
    "Configurons l'environnement Gym pour gridworld-v0. Il a besoin de 2 choses:\n",
    "\n",
    "- Un plan (le numéro 4 par exemple, qui est déjà assez complexe)\n",
    "- Des valeurs associées à chaque type de case. 0 correspond à une case vide, 1 correspond à\n",
    "un mur (pas de reward associé car impossible de s'y déplacer), 2 correspond au joueur,\n",
    "3 correspond à une case verte, 4 une case jaune, 5 une case rouge et 6 une case rose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"gridworld-v0\")\n",
    "outdir = 'TP3/qlearning-agent-results'\n",
    "envm = wrappers.Monitor(env, directory=outdir, force=True, video_callable=False)\n",
    "env.setPlan(\"gridworldPlans/plan2.txt\", {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1}) # initialiser le plan\n",
    "env.seed(0)\n",
    "lol = env.render() #visualiser le plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nous allons définir deux classes d'agent\n",
    "\n",
    "Un agent prenant des décisions complètement aléatoire, et un agent faisant du Q-learning. Cela servira comme point de comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import operator\n",
    "\n",
    "class Qagent(object):\n",
    "    \"\"\" Q-learning agent \"\"\"\n",
    "    def __init__(self, env, gamma=0.99, alpha=0.8, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.Q = {} # initialement vide: on ne connait rien\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.last_action = None\n",
    "\n",
    "    def register_state(self, state):\n",
    "        if state not in self.Q:\n",
    "            self.Q[state] = np.zeros(self.env.action_space.n)\n",
    "                \n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\" On parle pas chinois ici \"\"\"\n",
    "        state = self.env.state2str(observation)\n",
    "        self.register_state(state)\n",
    "        # epsilon-greedy:\n",
    "        if random.random() < self.epsilon:\n",
    "            # sampler une action au hasard\n",
    "            chosen_action = self.env.action_space.sample()\n",
    "        else:\n",
    "            # sinon, effectuer l'action conforme à la politique\n",
    "            chosen_action = np.argmax(self.Q[state])\n",
    "\n",
    "        self.last_action = chosen_action\n",
    "        self.last_state = state\n",
    "        return chosen_action\n",
    "\n",
    "    def improve(self, observation, reward, done):\n",
    "        \"\"\" Hassoul y'a qu'le charbon qui paie \"\"\"\n",
    "        new_state = self.env.state2str(observation)\n",
    "        self.register_state(new_state)\n",
    "        \n",
    "        LPBDTLR = self.gamma * self.Q[new_state][np.argmax(self.Q[new_state])]\n",
    "        self.Q[self.last_state][self.last_action] = ((1 - self.alpha) * self.Q[self.last_state][self.last_action]) + self.alpha * (reward + LPBDTLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0  rsum=0.9272712587250228, au bout de 133 actions\n",
      "episode 10  rsum=0.9039079604685804, au bout de 352 actions\n",
      "episode 20  rsum=0.9303422721525216, au bout de 122 actions\n",
      "episode 30  rsum=0.9041514247506817, au bout de 344 actions\n",
      "episode 40  rsum=0.9055709317994223, au bout de 307 actions\n",
      "episode 50  rsum=0.9143979674857962, au bout de 200 actions\n",
      "episode 60  rsum=0.9764719287203633, au bout de 28 actions\n",
      "episode 70  rsum=0.9150884412904268, au bout de 195 actions\n",
      "episode 80  rsum=0.9672282040983984, au bout de 41 actions\n",
      "episode 90  rsum=0.9453047981626173, au bout de 81 actions\n",
      "episode 100  rsum=0.9639823631203233, au bout de 46 actions\n",
      "episode 110  rsum=0.9615006067137537, au bout de 50 actions\n",
      "episode 120  rsum=0.982790693759723, au bout de 20 actions\n",
      "episode 130  rsum=0.9235948155533987, au bout de 148 actions\n",
      "episode 140  rsum=0.9579601202477159, au bout de 56 actions\n",
      "episode 150  rsum=0.9414731972678324, au bout de 90 actions\n",
      "episode 160  rsum=0.9772342714347104, au bout de 27 actions\n",
      "episode 170  rsum=0.9444231326791812, au bout de 83 actions\n",
      "episode 180  rsum=0.967897175856968, au bout de 40 actions\n",
      "episode 190  rsum=0.9198556845167377, au bout de 166 actions\n",
      "episode 200  rsum=0.96335253948912, au bout de 47 actions\n",
      "episode 210  rsum=0.9672282040983984, au bout de 41 actions\n",
      "episode 220  rsum=0.9659102628368402, au bout de 43 actions\n",
      "episode 230  rsum=0.9579601202477159, au bout de 56 actions\n",
      "episode 240  rsum=0.9742303369654397, au bout de 31 actions\n",
      "episode 250  rsum=0.9764719287203633, au bout de 28 actions\n",
      "episode 260  rsum=0.9525137117423803, au bout de 66 actions\n",
      "episode 270  rsum=0.9406677806422026, au bout de 92 actions\n",
      "episode 280  rsum=0.9905338254258717, au bout de 11 actions\n",
      "episode 290  rsum=0.9811630589539047, au bout de 22 actions\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "EPSILON = 0.1\n",
    "\n",
    "agent = Qagent(env, gamma=GAMMA, alpha=LEARNING_RATE, epsilon=EPSILON)\n",
    "\n",
    "nbruns = 300\n",
    "\n",
    "for i in range(nbruns):\n",
    "    obs = envm.reset()\n",
    "    rsum = 0\n",
    "    reward = 0\n",
    "    done = False\n",
    "    j = 0\n",
    "    while True:\n",
    "        action = agent.act(obs, reward, done)\n",
    "        obs, reward, done, _ = envm.step(action)\n",
    "        rsum = (rsum * GAMMA) + reward\n",
    "        j += 1\n",
    "        if done:\n",
    "            if i % 10 == 0:\n",
    "                print(\"episode\", i,\" rsum=\" + str(rsum) + \", au bout de \" + str(j) + \" actions\")\n",
    "            break\n",
    "        else:\n",
    "            agent.improve(obs, reward, done)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
