{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 3: Q-learning\n",
    "\n",
    "_Ismaël Bonneau_\n",
    "\n",
    "Ce TP sert de compte-rendu et d'explication à la fois.\n",
    "\n",
    "Algorithmes implémentés:\n",
    "\n",
    "- Q-learning en version basique\n",
    "- Q-learning en version SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gridworld\n",
    "from gym import wrappers, logger\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nous allons l'appliquer sur un jeu simple, GridWorld\n",
    "\n",
    "Il s'agit d'un petit jeu en labyrinthe simple dans lequel un personnage (carré) doit rejoindre un endroit d'arrivée et évitant les flammes. Un reward positif est donné à l'arrivée, un négatif au passage dans les flammes et un reward négatif très petit (${-0.01}$ par exemple est donné aux autres cases, cela pour limiter le temps passé à cheminer à traver le labyrinthe.\n",
    "\n",
    "Configurons l'environnement Gym pour gridworld-v0. Il a besoin de 2 choses:\n",
    "\n",
    "- Un plan (le numéro 4 par exemple, qui est déjà assez complexe)\n",
    "- Des valeurs associées à chaque type de case. 0 correspond à une case vide, 1 correspond à\n",
    "un mur (pas de reward associé car impossible de s'y déplacer), 2 correspond au joueur,\n",
    "3 correspond à une case verte, 4 une case jaune, 5 une case rouge et 6 une case rose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"gridworld-v0\")\n",
    "outdir = 'TP3/qlearning-agent-results'\n",
    "envm = wrappers.Monitor(env, directory=outdir, force=True, video_callable=False)\n",
    "env.setPlan(\"gridworldPlans/plan4.txt\", {0: -0.001, 3: 1, 4: 1, 5: -1, 6: -1}) # initialiser le plan\n",
    "env.seed(0)\n",
    "lol = env.render() #visualiser le plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nous allons définir deux classes d'agent\n",
    "\n",
    "Un agent prenant des décisions complètement aléatoire, et un agent faisant du Q-learning. Cela servira comme point de comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qagent(object):\n",
    "    \"\"\" Q-learning agent \"\"\"\n",
    "    def __init__(self, env, gamma=0.99, alpha=0.8, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.Q = {} # initialement vide: on ne connait rien\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.last_action = None\n",
    "\n",
    "    def register_state(self, observation):\n",
    "        if observation not in self.Q:\n",
    "            self.Q[observation] = {a: 0 for a in range(self.env.action_space.n)}\n",
    "                \n",
    "    def act(self, observation, reward, done):\n",
    "        \"\"\" On parle pas chinois ici \"\"\"\n",
    "        state = self.env.state2str(observation)\n",
    "        self.register_state(state)\n",
    "        # epsilon-greedy:\n",
    "        if random.random() < self.epsilon:\n",
    "            # sampler une action au hasard\n",
    "            chosen_action = self.env.action_space.sample()\n",
    "        else:\n",
    "            # sinon, effectuer l'action conforme à la politique\n",
    "            chosen_action = np.argmax(self.Q[state])\n",
    "\n",
    "        self.last_action = chosen_action\n",
    "        self.last_state = state\n",
    "        return chosen_action\n",
    "\n",
    "    def improve(self, observation, reward, done):\n",
    "        \"\"\" Hassoul y'a qu'le charbon qui paie \"\"\"\n",
    "        new_state = self.env.state2str(observation)\n",
    "        self.register_state(new_state)\n",
    "        LPBDTLR = self.gamma * max(self.Q[new_state].items(), key=operator.itemgetter(1))[1]\n",
    "        self.Q[self.last_state][self.last_action] += self.alpha * (reward + LPBDTLR - self.Q[self.last_state][self.last_action])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
