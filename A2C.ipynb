{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic methods\n",
    "\n",
    "## Principe d'actor critic\n",
    "\n",
    "blaaaablabla\n",
    "\n",
    "## A2C (l'algorithme que l'on va implémenter)\n",
    "\n",
    "blaaaablabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, num_actions, hidden_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.critic = nn.Sequential(nn.Linear(input_dim, hidden_size), nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1))\n",
    "\n",
    "        self.actor = nn.Sequential(nn.Linear(input_dim, hidden_size), nn.ReLU(), \n",
    "            nn.Linear(hidden_size, num_actions))\n",
    "\n",
    "    def forward(self, observation):\n",
    "        value = self.critic(observation)\n",
    "        d = 1\n",
    "        if len(observation.size()) == 1:\n",
    "            d = 0\n",
    "        policy_s = nn.functional.softmax(self.actor(observation), dim=d)\n",
    "\n",
    "        return value, policy_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appliquons cela sur un petit jeu simple, Mountain Car \n",
    "\n",
    "Doc et code source <a href=\"https://github.com/openai/gym/wiki/MountainCar-v0\">ici</a>\n",
    "\n",
    "Ce jeu est disponible dans l'environnement gym. Le principe est plutôt simple: faire grimper une colline à une voiture. La voiture n'est pas capable de monter en une seule fois: il faut avancer et reculer au fur et à mesure pour prendre de l'élan et réussir à monter. Le jeu se termine quand la voiture a atteint le drapeau, ou si la voiture n'a pas réussi après 200 itérations\n",
    "\n",
    "Ce jeu est modélisé par des états codés comme un vecteur en 2 dimensions modélisant 2 mesures physiques: position de la voiture en x, et vitesse. Trois actions possible: bouger d'un cran vers la droite, bouger d'un cran vers la gauche, ne rien faire.\n",
    "\n",
    "<img src=\"images/mountaincar.gif\" width=\"500\">\n",
    "\n",
    "Configurons l'environnement Mountain Car-V0 avec gym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation shape:  (2,)\n",
      "action space:  3\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "outdir = 'TP5/MountainCar-v0/A2C-agent-results'\n",
    "envm = gym.wrappers.Monitor(env, directory=outdir, force=True, video_callable=False)\n",
    "env.seed(0)\n",
    "env.verbose = False\n",
    "\n",
    "state = env.reset()\n",
    "print(\"observation shape: \", state.shape)\n",
    "print(\"action space: \", env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4ecfb7f1cfe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mac_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "NUM_EPISODES = 1000\n",
    "NUM_STEPS = 200 # 200 par défaut\n",
    "GAMMA = 0.99\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "hidden_layer_size = 32\n",
    "\n",
    "input_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "actor_critic = ActorCritic(input_dim, action_dim, hidden_layer_size)\n",
    "optimizer = torch.optim.Adam(actor_critic.parameters(), lr=learning_rate)\n",
    "\n",
    "## APPRENTISSAGE SUR DES SCENARIOS FINIS:\n",
    "## JOUER UN SCENARIO COMPLET, PUIS APPRENDRE DESSUS\n",
    "\n",
    "# jouer un NUM_EPISODES \"parties\"\n",
    "for episode in range(NUM_EPISODES):\n",
    "    logprobs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    # jouer au max NUM_STEPS \"coups\" pour cette partie\n",
    "    # ... avec la politique actuelle bien sûr\n",
    "    for steps in range(NUM_STEPS):\n",
    "        value, policy = actor_critic.forward(torch.from_numpy(state).float())\n",
    "        value = value.detach().numpy()[0]\n",
    "        policy_detach = policy.detach().numpy() \n",
    "        \n",
    "        # on sample une action d'apres la distribution de la politique courante pour cet état\n",
    "        chosen_action = np.random.choice(action_dim, p=policy_detach)\n",
    "        # calculer la log-prob de la politique pour l'action choisie\n",
    "        # pour pouvoir calculer le gradient par la suite\n",
    "        log_prob = torch.log(policy.squeeze(0)[chosen_action])\n",
    "        \n",
    "        new_state, reward, done, _ = env.step(chosen_action)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        logprobs.append(log_prob)\n",
    "        \n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done or steps == NUM_STEPS-1:\n",
    "            # Si on est à la fin de l'épisode...\n",
    "            Qval, _ = actor_critic.forward(torch.from_numpy(new_state).float())\n",
    "            Qval = Qval.detach().numpy()[0]\n",
    "\n",
    "    # compute Q values\n",
    "    Qvals = np.zeros_like(values)\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        Qval = rewards[t] + GAMMA * Qval  #etape 2 update Qval \n",
    "        Qvals[t] = Qval\n",
    "        \n",
    "    values = torch.FloatTensor(values)\n",
    "    Qvals = torch.FloatTensor(Qvals)\n",
    "    logprobs = torch.stack(logprobs)\n",
    "        \n",
    "    advantage = Qvals - values #calcul de A (étape 3)\n",
    "        \n",
    "    # compute policy loss \n",
    "    actor_loss = (-logprobs * advantage).mean()\n",
    "    # compute value loss (mse loss)\n",
    "    critic_loss = advantage.pow(2).mean()\n",
    "        \n",
    "    ac_loss = actor_loss + critic_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    ac_loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
