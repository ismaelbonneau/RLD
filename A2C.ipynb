{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic methods\n",
    "\n",
    "## Principe d'actor critic\n",
    "\n",
    "blaaaablabla\n",
    "\n",
    "## A2C (l'algorithme que l'on va implémenter)\n",
    "\n",
    "blaaaablabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, num_actions, hidden_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.critic = nn.Sequential(nn.Linear(input_dim, hidden_size), nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1))\n",
    "\n",
    "        self.actor = nn.Sequential(nn.Linear(input_dim, hidden_size), nn.Tanh(), \n",
    "            nn.Linear(hidden_size, num_actions))\n",
    "\n",
    "    def forward(self, observation):\n",
    "        value = self.critic(observation)\n",
    "        d = 1\n",
    "        if len(observation.size()) == 1:\n",
    "            d = 0\n",
    "        policy_s = nn.functional.softmax(self.actor(observation), dim=d)\n",
    "\n",
    "        return value, policy_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appliquons cela sur un petit jeu simple, Mountain Car \n",
    "\n",
    "Doc et code source <a href=\"https://github.com/openai/gym/wiki/MountainCar-v0\">ici</a>\n",
    "\n",
    "Ce jeu est disponible dans l'environnement gym. Le principe est plutôt simple: faire grimper une colline à une voiture. La voiture n'est pas capable de monter en une seule fois: il faut avancer et reculer au fur et à mesure pour prendre de l'élan et réussir à monter. Le jeu se termine quand la voiture a atteint le drapeau, ou si la voiture n'a pas réussi après 200 itérations\n",
    "\n",
    "Ce jeu est modélisé par des états codés comme un vecteur en 2 dimensions modélisant 2 mesures physiques: position de la voiture en x, et vitesse. Trois actions possible: bouger d'un cran vers la droite, bouger d'un cran vers la gauche, ne rien faire.\n",
    "\n",
    "<img src=\"images/mountaincar.gif\" width=\"500\">\n",
    "\n",
    "Configurons l'environnement Mountain Car-V0 avec gym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation shape:  (2,)\n",
      "action space:  3\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "outdir = 'TP5/MountainCar-v0/A2C-agent-results'\n",
    "envm = gym.wrappers.Monitor(env, directory=outdir, force=True, video_callable=False)\n",
    "env.seed(0)\n",
    "env.verbose = False\n",
    "\n",
    "state = env.reset()\n",
    "print(\"observation shape: \", state.shape)\n",
    "print(\"action space: \", env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 done in 0.23s reward:  -200.0\n",
      "episode 10 done in 0.22s reward:  -200.0\n",
      "episode 20 done in 0.25s reward:  -200.0\n",
      "episode 30 done in 0.22s reward:  -200.0\n",
      "episode 40 done in 0.23s reward:  -200.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-72c145c4976a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mstart_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mpolicy_detach\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-bf948d645762>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "NUM_EPISODES = 1000\n",
    "NUM_STEPS = 200 # 200 par défaut\n",
    "GAMMA = 0.99\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "hidden_layer_size = 64\n",
    "\n",
    "input_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "actor_critic = ActorCritic(input_dim, action_dim, hidden_layer_size)\n",
    "optimizer = torch.optim.Adam(actor_critic.parameters(), lr=learning_rate)\n",
    "\n",
    "roi_arouf = []\n",
    "nurmagomedov = []\n",
    "\n",
    "## APPRENTISSAGE SUR DES SCENARIOS FINIS:\n",
    "## JOUER UN SCENARIO COMPLET, PUIS APPRENDRE DESSUS\n",
    "\n",
    "start = time.time()\n",
    "# jouer un NUM_EPISODES \"parties\"\n",
    "for episode in range(NUM_EPISODES):\n",
    "    logprobs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    # jouer au max NUM_STEPS \"coups\" pour cette partie\n",
    "    # ... avec la politique actuelle bien sûr\n",
    "    start_episode = time.time()\n",
    "    for steps in range(NUM_STEPS):\n",
    "        value, policy = actor_critic.forward(torch.from_numpy(state).float())\n",
    "        value = value.detach().numpy()[0]\n",
    "        policy_detach = policy.detach().numpy() \n",
    "        \n",
    "        # on sample une action d'apres la distribution de la politique courante pour cet état\n",
    "        chosen_action = np.random.choice(action_dim, p=policy_detach)\n",
    "        # calculer la log-prob de la politique pour l'action choisie\n",
    "        # pour pouvoir calculer le gradient par la suite\n",
    "        log_prob = torch.log(policy.squeeze(0)[chosen_action])\n",
    "        \n",
    "        # effectuer l'action et récupérer le reward\n",
    "        new_state, reward, done, _ = env.step(chosen_action)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        logprobs.append(log_prob)\n",
    "        state = new_state\n",
    "        \n",
    "        if done or steps == NUM_STEPS-1:\n",
    "            # Si on est à la fin de l'épisode...\n",
    "            Qval, _ = actor_critic.forward(torch.from_numpy(new_state).float())\n",
    "            Qval = Qval.detach().numpy()[0]\n",
    "            \n",
    "            roi_arouf.append(np.sum(rewards))\n",
    "            nurmagomedov.append(steps)\n",
    "\n",
    "    # compute Q values\n",
    "    Qvals = np.zeros_like(values)\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        Qval = rewards[t] + GAMMA * Qval  #etape 2 update Qval \n",
    "        Qvals[t] = Qval\n",
    "        \n",
    "    values = torch.FloatTensor(values)\n",
    "    Qvals = torch.FloatTensor(Qvals)\n",
    "    logprobs = torch.stack(logprobs)\n",
    "        \n",
    "    advantage = Qvals - values #calcul de A (étape 3)\n",
    "    \n",
    "    ## Apprentissage\n",
    "    \n",
    "    # compute policy loss \n",
    "    actor_loss = (-logprobs * advantage).mean()\n",
    "    # compute value loss (mse loss)\n",
    "    critic_loss = advantage.pow(2).mean()\n",
    "        \n",
    "    ac_loss = actor_loss + critic_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    ac_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(\"episode {}\".format(episode)+\" done in {0:.2f}s\".format(time.time() - start_episode)+\n",
    "             \" reward: \", np.sum(rewards))\n",
    "    \n",
    "print(\"done in %f\" % (time.time() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
